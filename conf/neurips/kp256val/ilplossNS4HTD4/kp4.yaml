seed_everything: 42

data:
  class_path: src.gm_dataset.GMDataModule
  init_args:
    cfg_path: src/GraphMatching/experiments/spair.json
    num_keypoints: 4
    eval_samples: 256
    eval_on_test: False
    train_eval_samples: 128 
    eval_batch_size: 64
    num_workers:  4
    train_batch_size: 8
    train_epoch_iters: 400
    test_samples: -1

model:
  class_path: src.models.GMModel
  init_args:
    core:
      class_path: src.cores.SolverFreeCore
      init_args:
        encoder:
          class_path: src.CombOptNet.src.encoders.DisjointEncoder
          init_args:
            ab_encoders:
              - class_path: src.CombOptNet.src.encoders.StaticABEncoder
                init_args:
                  num_vars: 16
                  num_constrs: 8
                  batch_size_extractor: src.GraphMatching.utils.utils.get_batch_size
                  init_all_fn: src.CombOptNet.src.utils.initialize_static_constraints
                  init_all_args:
                    variable_range: {'lb': 0, 'ub': 1}
                    offset_sample_point: "random_unif"
                    request_offset_const: True
                    #feasible_point: origin

            c_encoder:
              class_path: src.encoders.KeyPointMatchingCEncoder
              init_args:
                backbone_params:
                  alpha: 0.0
        known_ab_encoder: 
          class_path: src.encoders.LUToABEncoder
          init_args:
            lu_encoder: 
              class_path: src.encoders.StaticLUEncoder
              init_args:
                batch_size_extractor: src.GraphMatching.utils.utils.get_batch_size
                num_vars: 16
                lb: -0.485
                ub: 1.485 
                #              class_path: src.CombOptNet.src.encoders.StaticABEncoder
                #              init_args:
                #                num_vars: 10
                #                num_constrs: 20


        sampler:
          class_path: src.samplers.SamplerList
          init_args:
            samplers:
              - class_path: src.samplers.BitNbrSampler

              - class_path: src.samplers.BitKHopSampler
                init_args:
                  num_hops: 2
                  num_samples: 16

              - class_path: src.samplers.BitKHopSampler
                init_args:
                  num_hops: 3
                  num_samples: 16 

              - class_path: src.samplers.BitKHopSampler
                init_args:
                  num_hops: 4
                  num_samples: 16

              - class_path: src.samplers.RandIntNbrWrapper
                init_args:
                  sampler:
                    class_path: src.samplers.ProjSampler

              - class_path: src.samplers.BatchSampler

            

        criterion:
          class_path: src.losses.ILPLoss
          init_args:
            pos_param: 0.01
            neg_param: 0.01
            criterion: hinge
            balancer:
              class_path: src.losses.CoVBalancer
              init_args:
                num_losses: 3

    solver:
      class_path: src.CombOptNet.src.ilp.CombOptNet
      init_args:
        vtype: 'B'
        env:
          class_path: gurobi.Env
          init_args:
            params:
              OutputFlag: 0
              TimeLimit: 10
        num_workers: 10

    optimizer:
      custom:
        '.*node_layers.*':
            lr: 2.5e-5
        '.*edge_layers.*':
            lr: 2.5e-5
        '.*final_layers.*':
            lr: 2.5e-5

      class_path: torch.optim.AdamW
      init_args:
        lr: 1.0e-3
        weight_decay: 0.0

    lr_scheduler:
      # class_path: torch.optim.lr_scheduler.CyclicLR
      # init_args:
      #   base_lr: 0.0
      #   max_lr: 1.0e-3
      #   step_size_up: 1000
      #   cycle_momentum: false
      # config:
      #   interval: step

      #class_path: torch.optim.lr_scheduler.ExponentialLR
      class_path: torch.optim.lr_scheduler.MultiStepLR
      init_args:
         gamma: 0.1
         milestones: [1000] 


        #lr_schedules = {
        #"long_halving": (10, (2, 4, 6, 8, 10), 0.5),
        #"short_halving": (2, (1,), 0.5),
        #"long_nodrop": (10, (10,), 1.0),
        #"minirun": (1, (10,), 1.0),
        #}

    schedulers:
        #      "temp":
        #        class_path: torch.optim.lr_scheduler.MultiStepLR
        #        init_args:
        #          gamma: 0.2
        #          milestones: [10,30]
        #          #milestones: [5,15]
        #        init: 0.5
      temp:
        class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
        init_args:
          mode: max
          factor: 0.1
          patience: 4
        config:
          monitor: val/acc/num
          interval: epoch
          frequency: 1
        init: 1.0


          #        class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
          #        init_args:
          #          mode: max
          #          factor: 0.2
          #          patience: 1000
          #        config:
          #          monitor: acc
          #          interval: step
          #init: 0.5



      # class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
      # init_args:
      #   mode: max
      #   factor: 0.3
      #   patience: 100
      #   min_lr: 0.000001
      # config:
      #   monitor: train/acc/bit_acc
      #   interval: step

skip_initial_validation: 1
trainer:
  max_epochs: 100
  callbacks:
    - class_path: src.callbacks.Timer
    - class_path: src.callbacks.CustomLogging
    - class_path: pytorch_lightning.callbacks.Timer
      init_args:
        duration: 01:00:00:00
        interval: step

    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/acc/num
        patience: 10
        mode: max
    - class_path: pytorch_lightning.callbacks.ModelSummary
      init_args:
        max_depth: -1
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step

    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: 'val/acc/num'
        mode: 'max'
        filename: best
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_on_train_epoch_end: True  
        save_last: True

 
  log_every_n_steps: 10
  check_val_every_n_epoch: 1
  logger:
    class_path: pytorch_lightning.loggers.TensorBoardLogger
    init_args:
      save_dir: runs/neurips/kp256val/ilplossNS4HTD/kp4
      name: vggasbb_splinit
      default_hp_metric: false
  # gradient_clip_val: 0.1
  gpus: 1
  precision: 16
  num_sanity_val_steps: 0
  reload_dataloaders_every_n_epochs: 0
