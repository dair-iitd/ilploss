seed_everything: 42

data:
  class_path: src.gm_dataset.GMDataModule
  init_args:
    cfg_path: src/GraphMatching/experiments/spair.json
    num_keypoints: 4
    eval_samples: 1024
    eval_on_test: False
    train_eval_samples: 128 
    eval_batch_size: 64
    num_workers:  4
    train_batch_size: 8
    train_epoch_iters: 400
    test_samples: -1


model:
  class_path: src.models.GMModel
  init_args:
    core:
      class_path: src.CombOptNet.src.cores.SolverCore
      init_args:
        encoder:
          class_path: src.CombOptNet.src.encoders.DisjointEncoder
          init_args:
            ab_encoders:
              - class_path: src.encoders.KeypointMatchingABEncoder
                init_args:
                  num_keypoints: 4
                  batch_size_extractor: src.GraphMatching.utils.utils.get_batch_size
            #
            c_encoder:
              class_path: src.encoders.KeyPointMatchingCEncoder
              init_args:
                backbone_params:
                  alpha: 1.0
        
        cost_train_criterion: torch.nn.BCEWithLogitsLoss
        cost_loss_wt: 1.0 

        solver:
          class_path: ext.wrapper.GraphMatchingWrapper
          init_args:
            solver_params:
              timeout: 1000
              primalComputationInterval: 10
              maxIter: 100
            lambda_val: 80.0
            #          class_path: src.CombOptNet.src.ilp.CombOptNet
            #          init_args:
            #            vtype: 'B'
            #            env:
            #              class_path: gurobi.Env
            #              init_args:
            #                params:
            #                  OutputFlag: 0
            #            num_workers: 10
            #            criterion:
            #              class_path: src.CombOptNet.src.ilp.ILPLoss
            #              init_args:
            #                tau: 0.5

        criterion:
          class_path: torch.nn.L1Loss

    solver:
      class_path: src.CombOptNet.src.ilp.CombOptNet
      init_args:
        vtype: 'B'
        env:
          class_path: gurobi.Env
          init_args:
            params:
              OutputFlag: 0
              TimeLimit: 10
        num_workers: 10

    optimizer:
      custom:
          #        '.*c_encoder.*': 
          #            lr: 2.5e-6
        '.*node_layers.*':
            lr: 2.5e-6
        '.*edge_layers.*':
            lr: 2.5e-6
        '.*final_layers.*':
            lr: 2.5e-6
      class_path: torch.optim.AdamW
      init_args:
        lr: 1.0e-4
        weight_decay: 0.0

    lr_scheduler:
      # class_path: torch.optim.lr_scheduler.CyclicLR
      # init_args:
      #   base_lr: 0.0
      #   max_lr: 1.0e-3
      #   step_size_up: 1000
      #   cycle_momentum: false
      # config:
      #   interval: step

      class_path: torch.optim.lr_scheduler.ExponentialLR
      #class_path: torch.optim.lr_scheduler.MultiStepLR
      init_args:
         #gamma: 0.5
         gamma: 1.0
         #milestones: [20,40,60,80,100] 


        #lr_schedules = {
        #"long_halving": (10, (2, 4, 6, 8, 10), 0.5),
        #"short_halving": (2, (1,), 0.5),
        #"long_nodrop": (10, (10,), 1.0),
        #"minirun": (1, (10,), 1.0),
        #}



      # class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
      # init_args:
      #   mode: max
      #   factor: 0.3
      #   patience: 100
      #   min_lr: 0.000001
      # config:
      #   monitor: train/acc/bit_acc
      #   interval: step

skip_initial_validation: 1
trainer:
  max_epochs: 100
  callbacks:
    - class_path: src.callbacks.Timer
    - class_path: src.callbacks.CustomLogging
    - class_path: pytorch_lightning.callbacks.Timer
      init_args:
        duration: 01:00:00:00
        interval: step

    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/acc/idv
        patience: 10
        mode: max
    - class_path: pytorch_lightning.callbacks.ModelSummary
      init_args:
        max_depth: -1
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step

    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: 'val/acc/num'
        mode: 'max'
        filename: best
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_on_train_epoch_end: True  
        save_last: True

 
  log_every_n_steps: 10
  check_val_every_n_epoch: 2
  logger:
    class_path: pytorch_lightning.loggers.TensorBoardLogger
    init_args:
      save_dir: runs/neurips/kp/neural/kp4
      name: vggasbb
      default_hp_metric: false
  # gradient_clip_val: 0.1
  gpus: 1
  precision: 16
  num_sanity_val_steps: 0
  reload_dataloaders_every_n_epochs: 0
